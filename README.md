# Pre-Training-BERT

In this repo, we have shown, how to pretrain a BERT model on both tasks, MLM (masked Language Modelling) and NSP (Next Sentence Prediction).
